{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Read in raw data\n",
    "data = pd.read_table(\"cmu_capstone2017_next_best_uber_upgrade_2m.txt\", sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:156: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Handling missing values\n",
    "data['etf_months_remaining'].fillna(0, inplace = True)\n",
    "data[\"bundle_tier\"]= data[\"bundle_tier\"].replace(\" \", \"Unknown\")\n",
    "data['contract_months_to_go'].fillna(0, inplace = True)\n",
    "data[\"contract_status\"]= data[\"contract_status\"].replace(\" \", \"Unknown\")\n",
    "data['contract_term_period'].fillna(0, inplace = True)\n",
    "data['customer_last_activity'].fillna(\"Unknown\", inplace = True)\n",
    "data['customer_residing_state'].fillna(\"Unknown\", inplace = True)\n",
    "data['customer_type'].fillna(\"Unknown\", inplace = True)\n",
    "data['directmail_mailed_l365'].fillna(0, inplace = True)\n",
    "data['discount_amt'].fillna(0, inplace = True)\n",
    "data['etf_declining_amt'].fillna(0, inplace = True)\n",
    "data['etf_declining_period'].fillna(0, inplace = True)\n",
    "data['etf_start_amt'].fillna(0, inplace = True)\n",
    "data['master_activity'].fillna(\"Unknown\", inplace = True)\n",
    "data['master_have_product3'].fillna(0, inplace = True)\n",
    "data['monthly_prev_product1_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_product2_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_product1_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_product3_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_product3_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_total_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_prev_total_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product1_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product1_type2_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product1_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product1_type3_pct'].fillna(0, inplace = True)\n",
    "data['monthly_product2_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product2_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product2_type3_pct'].fillna(0, inplace = True)\n",
    "data['monthly_product3_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product3_type2_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product3_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_product3_type3_pct'].fillna(0, inplace = True)\n",
    "data['monthly_total_type1_amt'].fillna(0, inplace = True)\n",
    "data['monthly_total_type3_amt'].fillna(0, inplace = True)\n",
    "data['monthly_total_type3_pct'].fillna(0, inplace = True)\n",
    "data['monthly_type1_amt'].fillna(0, inplace = True)\n",
    "data[\"offer_play_mix_name\"]= data[\"offer_play_mix_name\"].replace(\" \", \"No Offer\")\n",
    "data[\"offer_product_mix_name\"]= data[\"offer_product_mix_name\"].replace(\" \", \"No Offer\")\n",
    "data['product1_days_on_books'].fillna(0, inplace = True)\n",
    "data['product1_days_on_books'].fillna(0, inplace = True)\n",
    "data[\"product1_tier_name\"]= data[\"product1_tier_name\"].replace(\" \", \"Unknown\")\n",
    "data['product2_days_on_books'].fillna(0, inplace = True)\n",
    "data['product2_months_on_books'].fillna(0, inplace = True)\n",
    "data[\"product2_tier_name\"]= data[\"product2_tier_name\"].replace(\" \", \"Unknown\")\n",
    "data['product3_days_on_books'].fillna(0, inplace = True)\n",
    "data['product3_months_on_books'].fillna(0, inplace = True)\n",
    "data[\"product3_tier_name\"]= data[\"product3_tier_name\"].replace(\" \", \"Unknown\")\n",
    "data['product4_type2_amt'].fillna(0, inplace = True)\n",
    "data['times_polled'].fillna(0, inplace = True)\n",
    "data['total_type2_amt'].fillna(0, inplace = True)\n",
    "data[\"tp_bundle_name\"]= data[\"tp_bundle_name\"].replace(\" \", \"Unknown\")\n",
    "data['total_type2_amt'].fillna(0, inplace = True)\n",
    "data['video_action_adventure_usage'].fillna(0, inplace = True)\n",
    "data['video_animation_usage'].fillna(0, inplace = True)\n",
    "data['video_genre1_usage'].fillna(0, inplace = True)\n",
    "data['video_genre2_usage'].fillna(0, inplace = True)\n",
    "data['video_genre3_usage'].fillna(0, inplace = True)\n",
    "data['video_genre4_usage'].fillna(0, inplace = True)\n",
    "data['video_genre5_usage'].fillna(0, inplace = True)\n",
    "data['video_genre6_usage'].fillna(0, inplace = True)\n",
    "data['video_scifi_usage'].fillna(0, inplace = True)\n",
    "data['video_sports_total_usage'].fillna(0, inplace = True)\n",
    "data['video_sports1_usage'].fillna(0, inplace = True)\n",
    "data['video_sports2_usage'].fillna(0, inplace = True)\n",
    "data['video_sports3_usage'].fillna(0, inplace = True)\n",
    "data['video_sports4_usage'].fillna(0, inplace = True)\n",
    "data['video_sports5_usage'].fillna(0, inplace = True)\n",
    "data['video_sports6_usage'].fillna(0, inplace = True)\n",
    "data['video_sports7_usage'].fillna(0, inplace = True)\n",
    "data['video_sports8_usage'].fillna(0, inplace = True)\n",
    "data['video_sports9_usage'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type1_usage_l3m'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type1_usage_l6m'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type2_usage_l12m'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type2_usage_l3m'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type2_usage_l6m'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type1_usage_l12m'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type1_usage'].fillna(0, inplace = True)\n",
    "data['product3_metric1_type2_usage'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type1_l3m'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type1_l6m'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type2_usage'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type2_usage_l12m'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type2_usage_l3m'].fillna(0, inplace = True)\n",
    "data['product3_metric2_type2_usage_l6m'].fillna(0, inplace = True)\n",
    "data['product3_total_metric1_usage'].fillna(0, inplace = True)\n",
    "data['offer_months_duration'].fillna(0, inplace = True)\n",
    "data['monthly_total_amt'].fillna(0, inplace = True)\n",
    "data['internet_metric1_usage_l12m'].fillna(0, inplace = True)\n",
    "data['internet_metric1_usage_l3m'].fillna(0, inplace = True)\n",
    "data['internet_total_usage'].fillna(0, inplace = True)\n",
    "data['internet_metric2_usage'].fillna(0, inplace = True)\n",
    "data['internet_metric2_usage_l6m'].fillna(0, inplace = True)\n",
    "data['monthly_amt2'].fillna(0, inplace = True)\n",
    "data['monthly_avg_type1_l6m'].fillna(0, inplace = True)\n",
    "data['monthly_total_avg_l12m'].fillna(0, inplace = True)\n",
    "data['monthly_total_min_l12m'].fillna(0, inplace = True)\n",
    "data['product2_type2_amt'].fillna(0, inplace = True)\n",
    "data['product1_months_on_books'].fillna(0, inplace = True)\n",
    "data['x1_months_on_books'].fillna(0, inplace = True)\n",
    "data['disconnect_type'].fillna(\"Unknown\", inplace = True)\n",
    "data['video_homepassed'].fillna(\"Unknown\", inplace = True)\n",
    "data['pay_ind'].fillna(\"Not Applied\", inplace = True)\n",
    "data['prev_number_of_products'].fillna(0, inplace = True)\n",
    "data['months_since_last_had_product1'].fillna(0, inplace = True)\n",
    "data['months_since_last_had_product2'].fillna(0, inplace = True)\n",
    "data['months_since_last_had_product3'].fillna(0, inplace = True)\n",
    "data['monthly_net_offer_type1_amt'].fillna(0, inplace = True)\n",
    "demo2_good = data[(data['demo2'] != 'U') & (data['demo2'].isnull() == False)]['demo2']\n",
    "demo2_median = np.median(demo2_good.astype(np.float64))\n",
    "data['demo2'].fillna(demo2_median, inplace = True)\n",
    "data[\"demo2\"] = data[\"demo2\"].replace('U', demo2_median)\n",
    "data['demo2'] = data['demo2'].astype(np.float64)\n",
    "\n",
    "# Drop unused columns\n",
    "data.drop('entity_name', axis=1,inplace=True)\n",
    "data.drop('region_code', axis=1,inplace=True)\n",
    "data.drop('tenure_by_days', axis=1, inplace=True)\n",
    "data.drop('video_total_usage', axis=1, inplace=True)\n",
    "data.drop('x1_platform', axis=1, inplace=True)\n",
    "\n",
    "# From now on, only use residential data\n",
    "res = data[data['customer_type']=='RESIDENTIAL']\n",
    "\n",
    "# Selected features by LASSO, 33 features + target\n",
    "selected_features = [\"id\", \"bundle_tier\", \"contract_status\", \"customer_last_activity\", \"customer_residing_state\", \n",
    "                     \"downgraded_l12m\", \"downgradeinlast6\", \"email_provided\", \"etf_declining_period\", \"final_sales_channel\",\n",
    "                    \"had_product3_l12m\", \"have_product1\", \"master_activity\", \"monthly_product1_type2_amt\", \n",
    "                     \"monthly_product1_type3_pct\", \"monthly_product3_type2_amt\", \"monthly_product3_type3_pct\",\n",
    "                    \"number_of_products\", \"offer_play_mix_name\", \"offer_product_mix_name\", \"onboarding_sales_channel\",\n",
    "                    \"prev_ending_product_mix\", \"prev_product_mix\", \"prev_product1_tier_name\", \"prev_product2_tier_name\",\n",
    "                    \"product_mix\", \"product1_tier_name\", \"product2_indicator\", \"product2_tier_name\",\n",
    "                    \"product2_type2_amt\", \"product3_tier_name\", \"product3_unlimited_plan\", \"product4_type2_amt\",\n",
    "                    \"tp_bundle_name\", \"target\", \"target_m3_product_mix\",'tenure_by_account']\n",
    "\n",
    "filtered_data = res[selected_features]\n",
    "\n",
    "# One-hot encoxding\n",
    "def convert_dummie(x,y):\n",
    "    dummies = pd.get_dummies(x[y])\n",
    "    cols = dummies.columns.values\n",
    "    for i in range(len(cols)):\n",
    "        cols[i]=y + '_'+ cols[i]\n",
    "    dummies.columns = [cols]\n",
    "    return dummies\n",
    "\n",
    "cat_data = filtered_data.select_dtypes(exclude=['int64', 'float64'])\n",
    "dummy_col = []\n",
    "for i in range(len(cat_data.columns)-1):\n",
    "    dummy_col.append(convert_dummie(cat_data, cat_data.columns[i]))    \n",
    "dummy_df = pd.concat(dummy_col, axis = 1, join=\"inner\")\n",
    "\n",
    "for name in cat_data.columns:\n",
    "    if name != \"target_m3_product_mix\":\n",
    "        filtered_data.drop(name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df = pd.concat([filtered_data, dummy_df], axis=1, join=\"inner\")\n",
    "\n",
    "# Include 3 self-created features\n",
    "df['product_mix_ranking'] = 0\n",
    "df['product_mix_ranking'] = np.where(df['product_mix_Video/Internet/Voice'] == 1, 1,df['product_mix_ranking'])\n",
    "df['product_mix_ranking'] = np.where(df['product_mix_Video Only'] == 1, 2,df['product_mix_ranking'])\n",
    "df['product_mix_ranking'] = np.where(df['product_mix_Video/Internet'] == 1, 4,df['product_mix_ranking'])\n",
    "df['product_mix_ranking'] = np.where(df['product_mix_Internet Only'] == 1, 3,df['product_mix_ranking'])\n",
    "\n",
    "df['tenure_flag']=0\n",
    "df['tenure_flag']=np.where(((df['tenure_by_account']>=10) & (df['tenure_by_account']<=12)) | ((df['tenure_by_account']>=22) \n",
    "                                                                                              & (df['tenure_by_account']<=24)), \n",
    "                           1, df['tenure_flag'])\n",
    "\n",
    "df['nproducts_flag'] = 0\n",
    "df['nproducts_flag'] = np.where(df['number_of_products']<=2, 1, df['nproducts_flag'])\n",
    "\n",
    "\n",
    "df.drop('tenure_by_account', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a function predict class given probability X and threshold value y\n",
    "def predict_class(x,y):\n",
    "    if(x[1] > y):\n",
    "        return 1;\n",
    "    else:\n",
    "        return 0;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the threshold you would like to use to define an upgrade (from 0 to 1):\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Please enter the threshold you would like to use to define an upgrade (from 0 to 1):\")\n",
    "threshold = float(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'target_m3_product_mix': 'target_mix'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Taking out of bag test sample\n",
    "X = df\n",
    "#Here we are not dropping target and target_mix from X because we drop it later on during the training \n",
    "cols = ['target','target_mix','id']\n",
    "y = df[cols]\n",
    "\n",
    "#X_test and y_test will be the out of bag sample and X_train and y_train would be used (by splitting further into train and test set) for the modeling purposes.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mengdietu/Documents/Anaconda/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:36: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=50, n_jobs=1, oob_score=False, random_state=10,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest\n",
    "data2_upgrades = X_train[X_train['target'] == 1]\n",
    "\n",
    "data2_not_upgrades = X_train[X_train['target'] == 0]\n",
    "not_upgrade_sample = data2_not_upgrades.sample((np.shape(data2_upgrades)[0]), replace=True)\n",
    "balanced_Dataset = pd.concat([not_upgrade_sample,data2_upgrades])\n",
    "X_bal = balanced_Dataset\n",
    "y_bal = balanced_Dataset[cols]\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_bal, y_bal, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "#Datasets for training SVM\n",
    "X_train_secondStage = X_train_bal[X_train_bal['target'] == 1]\n",
    "y_train_secondStage = X_train_secondStage['target_mix']\n",
    "X_train_secondStage.drop('target', axis = 1, inplace = True)\n",
    "X_train_secondStage.drop('target_mix', axis = 1, inplace = True)\n",
    "X_train_secondStage.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "clf0 = CalibratedClassifierCV(LinearSVC(random_state=0), cv=2)\n",
    "modelSVM = OneVsRestClassifier(clf0)\n",
    "modelSVM.fit(X_train_secondStage, y_train_secondStage)\n",
    "\n",
    "#Dataset for training random forest\n",
    "X_train_bal = X_train_bal.drop('target', axis=1)\n",
    "X_train_bal = X_train_bal.drop('target_mix', axis=1)\n",
    "X_train_bal = X_train_bal.drop('id', axis=1)\n",
    "X_test_bal = X_test_bal.drop('target', axis=1)\n",
    "X_test_bal = X_test_bal.drop('target_mix', axis=1)\n",
    "X_test_bal = X_test_bal.drop('id', axis=1)\n",
    "y_train_bal.drop('target_mix', axis = 1, inplace = True)\n",
    "y_train_bal.drop('id', axis = 1, inplace = True)\n",
    "y_test_bal_rf = y_test_bal['target']\n",
    "y_test_bal_svm = y_test_bal['target_mix']\n",
    "\n",
    "model2 = RandomForestClassifier(n_estimators=50, random_state=10)\n",
    "model2.fit(X_train_bal, y_train_bal)\n",
    "# #This will hold probabilities for being a 0 (non- upgrade) or 1 (upgrade)\n",
    "# probs = model2.predict_proba(X_test_bal)\n",
    "# upgrade_Probabilities = pd.DataFrame(probs, columns = model2.classes_)\n",
    "# upgrade_Probabilities['id'] = y_test_bal.reset_index()['id']\n",
    "# #This holds our decision on whether or not he will upgrade\n",
    "# predicted_bal = [predict_class(probs[i], threshold) for i in range(len(probs))]\n",
    "# predicted_bal_df = pd.DataFrame(predicted_bal)\n",
    "# predicted_bal_df['id'] = y_test_bal.reset_index()['id']\n",
    "# #predicted labels will be our suggestion on what bundle he will upgrade to\n",
    "# predicted_lables = modelSVM.predict(X_test_bal)\n",
    "# predicted_lables_df = pd.DataFrame(predicted_lables)\n",
    "# predicted_lables_df['id'] =  y_test_bal.reset_index()['id']\n",
    "# #hold an array of probabilities for each product mix\n",
    "# predicted_proba = modelSVM.predict_proba(X_test_bal)\n",
    "# bundle_Probabilities = pd.DataFrame(predicted_proba, columns= modelSVM.classes_)\n",
    "# bundle_Probabilities['id'] = y_test_bal.reset_index()['id']\n",
    "\n",
    "\n",
    "# # generate evaluation metrics\n",
    "# print(\"---------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# print(\"------------------------------test set eval metrics------------------------------------------\")\n",
    "    \n",
    "# print(metrics.precision_score(y_test_bal_rf, predicted_bal))\n",
    "# print(metrics.confusion_matrix(y_test_bal_rf, predicted_bal))\n",
    "# print(metrics.classification_report(y_test_bal_rf, predicted_bal))\n",
    "# #pd.DataFrame(model2.feature_importances_).to_csv('feature_importance.csv')\n",
    "\n",
    "\n",
    "# f1 = metrics.fbeta_score(y_test_bal_rf,predicted_bal,average='binary',beta=0.5)\n",
    "\n",
    "# print(\"FBeta Score: \",f1)\n",
    "\n",
    "# comparisonTest = pd.DataFrame(predicted_lables)\n",
    "# comparisonTest['Real']=y_test_bal_svm.reset_index()['target_mix']\n",
    "# comparisonTest['target']=y_test_bal_rf.reset_index()['target']\n",
    "# comparisonTest['Equal']=[1 if ((comparisonTest['target'][i] == 1) & (comparisonTest[0][i]==comparisonTest['Real'][i])) else 0 for i in range(len(comparisonTest))]\n",
    "# print(sum(comparisonTest['Equal'])/sum(comparisonTest['target']))\n",
    "\n",
    "\n",
    "# #training output file\n",
    "# train_op = upgrade_Probabilities.merge(predicted_bal_df, on= ['id']).merge(predicted_lables_df,  on= ['id']).merge(bundle_Probabilities, on= ['id'])\n",
    "# train_op = train_op.rename(columns={'0_x': 'prob_Non_Upgrade', 1: 'prob_Upgrade', '0_y':'upgrade_Decision', 0:'recommended_product_mix'})\n",
    "# train_op.to_csv('train_output_file.txt',sep =\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenameSVM = 'finalized_model_SVM.sav'\n",
    "pickle.dump(modelSVM, open(filenameSVM, 'wb'))\n",
    "filenameRF = 'finalized_model_RF.sav'\n",
    "pickle.dump(model2, open(filenameRF, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
